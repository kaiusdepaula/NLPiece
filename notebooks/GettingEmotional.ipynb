{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's time to get emotional!\n",
    "\n",
    "The title may already give the vibe of what I'll try to achieve on this section: let's conduct a emotion detection analysis on raw texts of One Piece!\n",
    "\n",
    "The idea here is to try grasping at the general \"emotion\" (relief, sadness, happiness...) of a given group of tokens. We could conduct this analysis at a word level, but it's not a good idea, here is a good example of why:\n",
    "\n",
    "\"I'm very happy\" -> happiness\n",
    "\n",
    "\"I'm not happy\" -> sadness\n",
    "\n",
    "Breaking that string on a word level, we are opening for some inconsistencies, as the whole sentence has to be taken into consideration.\n",
    "\n",
    "A good thing here is that when I developed the whole system for grouping transcriptions, I knew I had to break it all down to a common token to know when a sentence would end. On this case, all sentences end with \".\"!\n",
    "\n",
    "*Quick disclaimer: even though I did it this way, spacy has a handy way of detecting when phrases start and end!*\n",
    "\n",
    "**Another huge disclaimer: I began writing this section as \"SENTIMENTAL\", but sentiment analysis is not really a good application here as all my transcripts are based on the same principle of a story being told and I'd have to deal with heavy inconsistencies and a lot of bias. Because of this, I decided to change the topic to \"emotion\" detection on the sentence level. This way, I may consider the general \"sentiment\" of every phrase used on each given group.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaius/Documents/NLPiece/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "amdgpu.ids: No such file or directory\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>text</th>\n",
       "      <th>saga</th>\n",
       "      <th>saga_expanded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>why dont you do it youra self. you dont have t...</td>\n",
       "      <td>East Blue</td>\n",
       "      <td>Romance Dawn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>buggys cannons blew the village to smtherens. ...</td>\n",
       "      <td>East Blue</td>\n",
       "      <td>Orange Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>could they have finished with him already. he ...</td>\n",
       "      <td>East Blue</td>\n",
       "      <td>Orange Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>your swords. before we fight id better. they c...</td>\n",
       "      <td>East Blue</td>\n",
       "      <td>Syrup Village</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>hes the cannon man. brother they say. captured...</td>\n",
       "      <td>East Blue</td>\n",
       "      <td>Baratie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>better take it easy. whos krieg. then fight me...</td>\n",
       "      <td>East Blue</td>\n",
       "      <td>Baratie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>ha you can tell youre a green horn. b bones. p...</td>\n",
       "      <td>East Blue</td>\n",
       "      <td>Baratie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>hatchan hachi arlongs first mate. if some thin...</td>\n",
       "      <td>East Blue</td>\n",
       "      <td>Arlong Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>having seen the strong hold of this pirate. go...</td>\n",
       "      <td>East Blue</td>\n",
       "      <td>Arlong Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>is the maximum you can hold you have no chance...</td>\n",
       "      <td>East Blue</td>\n",
       "      <td>Arlong Park</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   volume                                               text       saga  \\\n",
       "0       1  why dont you do it youra self. you dont have t...  East Blue   \n",
       "1       2  buggys cannons blew the village to smtherens. ...  East Blue   \n",
       "2       3  could they have finished with him already. he ...  East Blue   \n",
       "3       4  your swords. before we fight id better. they c...  East Blue   \n",
       "4       5  hes the cannon man. brother they say. captured...  East Blue   \n",
       "5       6  better take it easy. whos krieg. then fight me...  East Blue   \n",
       "6       7  ha you can tell youre a green horn. b bones. p...  East Blue   \n",
       "7       8  hatchan hachi arlongs first mate. if some thin...  East Blue   \n",
       "8       9  having seen the strong hold of this pirate. go...  East Blue   \n",
       "9      10  is the maximum you can hold you have no chance...  East Blue   \n",
       "\n",
       "   saga_expanded  \n",
       "0   Romance Dawn  \n",
       "1    Orange Town  \n",
       "2    Orange Town  \n",
       "3  Syrup Village  \n",
       "4        Baratie  \n",
       "5        Baratie  \n",
       "6        Baratie  \n",
       "7    Arlong Park  \n",
       "8    Arlong Park  \n",
       "9    Arlong Park  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n",
    "\n",
    "\n",
    "def classify_emotions(texts: list[str], threshold=0.3) -> Counter:\n",
    "    \"\"\"\n",
    "    Returns a Counter object with all emotions detected across a batch of sentences.\n",
    "\n",
    "    Parameters:\n",
    "        texts (list): List of text strings.\n",
    "        threshold (float): Minimum score to consider an emotion as detected.\n",
    "\n",
    "    Returns:\n",
    "        Counter: Aggregated count of detected emotions across all texts.\n",
    "    \"\"\"\n",
    "    cnt = Counter()\n",
    "    results = classifier(texts)  # One big call to the pipeline\n",
    "\n",
    "    for result in results:\n",
    "        cnt.update(\n",
    "            emotion['label']\n",
    "            for emotion in result\n",
    "            if emotion['score'] >= threshold\n",
    "        )\n",
    "\n",
    "    return cnt\n",
    "\n",
    "onepiecedata = pd.read_parquet(\"../outputs/volume.parquet\")\n",
    "onepiecedata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>saga</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabasta</td>\n",
       "      <td>here we go. theyre not monsters theyre welcomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon Lily</td>\n",
       "      <td>their clothes are thrown into a bubbling caudr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dressrosa</td>\n",
       "      <td>but we call bla. where do you get that power. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>East Blue</td>\n",
       "      <td>why dont you do it youra self. you dont have t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Enies Lobby</td>\n",
       "      <td>neeeight. no it doesnt matter i still wont ret...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          saga                                               text\n",
       "0     Alabasta  here we go. theyre not monsters theyre welcomi...\n",
       "1  Amazon Lily  their clothes are thrown into a bubbling caudr...\n",
       "2    Dressrosa  but we call bla. where do you get that power. ...\n",
       "3    East Blue  why dont you do it youra self. you dont have t...\n",
       "4  Enies Lobby  neeeight. no it doesnt matter i still wont ret..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onepiecedata_by_saga = onepiecedata.groupby(\"saga\")[\"text\"].apply(lambda x: \". \".join(x)).reset_index()\n",
    "onepiecedata_by_saga.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how this works with a quick example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' hes in prison because hes not a good guy',\n",
       " ' ueutenant i dont think thats a good idea',\n",
       " ' he made you look like a weaking why didnt you fight him',\n",
       " ' ill tell my father on you',\n",
       " ' is that right i guess i dont under stand']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onepiecedata[\"text\"][0].split(\".\")[3:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaius/Documents/NLPiece/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125: UserWarning: Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:310.)\n",
      "  return F.linear(input, self.weight, self.bias)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'neutral': 3, 'disappointment': 1, 'disapproval': 1, 'confusion': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_emotions(onepiecedata[\"text\"][0].split(\".\")[3:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds good! Let's see how it'll work when we aggregate it by sagas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m onepiecedata_by_saga[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43monepiecedata_by_saga\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassify_emotions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m onepiecedata_by_saga\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../outputs/emotions_by_saga.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m onepiecedata_by_saga[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m onepiecedata_by_saga[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mclassify_emotions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m onepiecedata_by_saga\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../outputs/emotions_by_saga.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m, in \u001b[0;36mclassify_emotions\u001b[0;34m(texts, threshold)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mReturns a Counter object with all emotions detected across a batch of sentences.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    Counter: Aggregated count of detected emotions across all texts.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m cnt \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[0;32m---> 28\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# One big call to the pipeline\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     31\u001b[0m     cnt\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m     32\u001b[0m         emotion[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m emotion \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m emotion[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold\n\u001b[1;32m     35\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:159\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[0;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1343\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1340\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1341\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1342\u001b[0m     )\n\u001b[0;32m-> 1343\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:1269\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1268\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1269\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:190\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    189\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/NLPiece/.venv/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:1320\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1320\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1332\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# onepiecedata_by_saga[\"emotions\"] = onepiecedata_by_saga[\"text\"].apply(lambda x: classify_emotions(x.split(\".\")))\n",
    "# onepiecedata_by_saga.to_parquet(\"../outputs/emotions_by_saga.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the process took a loooong time to execute as there is a lot of sentences for it to classify. As I've done it previously, I'll cache the results so I won't need to wait for the execution everytime I return to this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onepiecedata_by_saga = pd.read_parquet(\"../outputs/emotions_by_saga.parquet\")\n",
    "onepiecedata_by_saga.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll transform this data into a more visualization friendly format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_expanded = []\n",
    "for idx, row in onepiecedata_by_saga.iterrows():\n",
    "    for emotion, count in row[\"emotions\"].items():\n",
    "        emotion_expanded.append({\n",
    "            \"saga\": row[\"saga\"],\n",
    "            \"emotion\": emotion,\n",
    "            \"count\": count\n",
    "        })\n",
    "\n",
    "emotions_df = pd.DataFrame(emotion_expanded).fillna(0)\n",
    "emotions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, of course, we can remove the neutral emotions as they don't hold much value to this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_df = emotions_df[emotions_df[\"emotion\"] != \"neutral\"]\n",
    "emotions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And right away I'll group emotions. Why is that? The classifier has 26 potential classes and a neutral one. To look at each of them individually is quite a lot, so I'll group them by the following classes.\n",
    "\n",
    "\n",
    "### 🟥 1. **Anger / Conflict**\n",
    "Includes:\n",
    "- `anger`\n",
    "- `annoyance`\n",
    "- `disapproval`\n",
    "\n",
    "➡️ **Group label:** `Anger`\n",
    "\n",
    "\n",
    "### 🟩 2. **Joy / Positive Engagement**\n",
    "Includes:\n",
    "- `joy`\n",
    "- `amusement`\n",
    "- `excitement`\n",
    "- `approval`\n",
    "- `love`\n",
    "- `gratitude`\n",
    "- `pride`\n",
    "- `optimism`\n",
    "- `admiration`\n",
    "\n",
    "➡️ **Group label:** `Joy`\n",
    "\n",
    "\n",
    "### 🟦 3. **Sadness / Loss**\n",
    "Includes:\n",
    "- `sadness`\n",
    "- `disappointment`\n",
    "- `grief`\n",
    "- `remorse`\n",
    "\n",
    "➡️ **Group label:** `Sadness`\n",
    "\n",
    "\n",
    "### 🟨 4. **Fear / Insecurity**\n",
    "Includes:\n",
    "- `fear`\n",
    "- `nervousness`\n",
    "- `embarrassment`\n",
    "\n",
    "➡️ **Group label:** `Fear`\n",
    "\n",
    "\n",
    "### 🟪 5. **Surprise / Uncertainty**\n",
    "Includes:\n",
    "- `realization`\n",
    "- `relief`\n",
    "- `surprise`\n",
    "- `curiosity`\n",
    "- `confusion`\n",
    "\n",
    "➡️ **Group label:** `Surprise`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_groups = {\n",
    "    'anger': 'Anger', 'annoyance': 'Anger', 'disapproval': 'Anger',\n",
    "\n",
    "    'joy': 'Joy', 'amusement': 'Joy', 'excitement': 'Joy',\n",
    "    'approval': 'Joy', 'love': 'Joy', 'gratitude': 'Joy',\n",
    "    'pride': 'Joy', 'optimism': 'Joy', 'admiration': 'Joy',\n",
    "\n",
    "    'sadness': 'Sadness', 'disappointment': 'Sadness',\n",
    "    'grief': 'Sadness', 'remorse': 'Sadness',\n",
    "\n",
    "    'fear': 'Fear', 'nervousness': 'Fear', 'embarrassment': 'Fear',\n",
    "\n",
    "    'realization': 'Surprise', 'relief': 'Surprise',\n",
    "    'surprise': 'Surprise', 'curiosity': 'Surprise',\n",
    "    'confusion': 'Surprise', 'neutral': 'Surprise'\n",
    "}\n",
    "\n",
    "# Add a new column with grouped emotion labels\n",
    "emotions_df[\"emotion_group\"] = emotions_df[\"emotion\"].map(emotion_groups)\n",
    "\n",
    "# Group by saga and emotion group, summing the counts\n",
    "grouped_emotions = (\n",
    "    emotions_df.groupby([\"saga\", \"emotion_group\"])[\"count\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate total emotion counts per saga\n",
    "total_per_saga = grouped_emotions.groupby(\"saga\")[\"count\"].sum().rename(\"total\")\n",
    "\n",
    "# Merge totals back and compute proportional share\n",
    "grouped_emotions = grouped_emotions.merge(total_per_saga, on=\"saga\")\n",
    "grouped_emotions[\"proportion\"] = grouped_emotions[\"count\"] / grouped_emotions[\"total\"]\n",
    "grouped_emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are sagas bigger in size, I'll make it all proportional to each saga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table for the stacked bar plot\n",
    "pivot_df = grouped_emotions.pivot_table(\n",
    "    index=\"saga\", \n",
    "    columns=\"emotion_group\", \n",
    "    values=\"proportion\", \n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Custom color palette for emotion groups\n",
    "emotion_colors = {\n",
    "    \"Joy\": \"#FFEC51\",\n",
    "    \"Anger\": \"#BF211E\",\n",
    "    \"Sadness\": \"#276FBF\",\n",
    "    \"Fear\": \"#20063B\",\n",
    "    \"Surprise\": \"#BFDBF7\"  \n",
    "}\n",
    "\n",
    "saga_order = [\n",
    "    \"East Blue\", \"Alabasta\", \"Skypiea\", \"Water 7\", \"Enies Lobby\",\n",
    "    \"Thriller Bark\", \"Sabaody\", \"Amazon Lily\", \"Impel Down\",\n",
    "    \"Marineford\", \"Post-War\", \"Fish-Man Island\", \"Punk Hazard\",\n",
    "    \"Dressrosa\", \"Zou\", \"Whole Cake Island\", \"Wano\"\n",
    "][::-1]\n",
    "\n",
    "# 2. Reindex pivot_df to reflect saga narrative order\n",
    "pivot_df = pivot_df.reindex(saga_order)\n",
    "\n",
    "\n",
    "# Ensure consistent order of columns\n",
    "pivot_df = pivot_df[[col for col in emotion_colors if col in pivot_df.columns]]\n",
    "\n",
    "# Plot horizontal stacked bar\n",
    "ax = pivot_df.plot(\n",
    "    kind=\"barh\",  # ← horizontal bar chart\n",
    "    stacked=True,\n",
    "    figsize=(14, 12),\n",
    "    color=[emotion_colors[col] for col in pivot_df.columns]\n",
    ")\n",
    "\n",
    "# Titles and labels\n",
    "plt.xlabel(\"Predominance of Emotion Group\", fontsize=15)\n",
    "plt.ylabel(\"\")\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Legend\n",
    "plt.legend(\n",
    "    title=\"Emotion Group\",\n",
    "    title_fontsize=15,\n",
    "    fontsize=13,\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.07),\n",
    "    ncol=len(pivot_df.columns)\n",
    ")\n",
    "\n",
    "# Layout and style\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "ax.set_xticklabels([])\n",
    "plt.grid(axis='x', linestyle='dotted', alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Notice how difficult is to compare the inner emotion groups. This happens because the stacked bar visual is not used for comparison and is only a way of looking to each saga individually. To compare them, we must use filters for each emotion group and order by the % predominance of that group. Let's do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Melt the pivot_df to long format\n",
    "melted_df = pivot_df.reset_index().melt(\n",
    "    id_vars=\"saga\",\n",
    "    var_name=\"emotion_group\",\n",
    "    value_name=\"proportion\"\n",
    ")\n",
    "\n",
    "# 2. Sort sagas by emotion-specific descending proportion\n",
    "melted_df[\"saga_ordered\"] = melted_df.groupby(\"emotion_group\")[\"proportion\"] \\\n",
    "    .transform(lambda x: pd.Series(x).rank(ascending=False, method='first'))\n",
    "\n",
    "# 3. Define the color mapping (same as you already use)\n",
    "emotion_colors = {\n",
    "    \"Joy\": \"#FFEC51\",\n",
    "    \"Anger\": \"#BF211E\",\n",
    "    \"Sadness\": \"#276FBF\",\n",
    "    \"Fear\": \"#20063B\",\n",
    "    \"Surprise\": \"#BFDBF7\"\n",
    "}\n",
    "\n",
    "# 4. Create the FacetGrid\n",
    "g = sns.FacetGrid(\n",
    "    data=melted_df,\n",
    "    col=\"emotion_group\",\n",
    "    sharex=False,\n",
    "    sharey=False,\n",
    "    col_wrap=2,\n",
    "    height=5,\n",
    "    aspect=1.3\n",
    ")\n",
    "\n",
    "# 5. Plot each facet with the color tied to its emotion\n",
    "for ax, (emotion, subdf) in zip(g.axes.flat, melted_df.groupby(\"emotion_group\")):\n",
    "    # Order sagas for this emotion only\n",
    "    ordered_sagas = subdf.sort_values(\"proportion\", ascending=False)[\"saga\"]\n",
    "\n",
    "    sns.barplot(\n",
    "        data=subdf,\n",
    "        y=\"saga\",\n",
    "        x=\"proportion\",\n",
    "        order=ordered_sagas,\n",
    "        color=emotion_colors[emotion],\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_title(emotion, fontsize=14)\n",
    "    ax.set_xlabel(\"Proportion\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "    # Optional: adjust font size\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "# Layout and style\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure 'saga' is a categorical column with defined order\n",
    "melted_df[\"saga\"] = pd.Categorical(melted_df[\"saga\"], categories=saga_order[::-1], ordered=True)\n",
    "melted_df = melted_df.sort_values(\"saga\")\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "for emotion in melted_df[\"emotion_group\"].unique():\n",
    "    df_emotion = melted_df[melted_df[\"emotion_group\"] == emotion]\n",
    "    plt.plot(\n",
    "        df_emotion[\"saga\"],\n",
    "        df_emotion[\"proportion\"],\n",
    "        label=emotion,\n",
    "        color=emotion_colors.get(emotion, \"gray\"),\n",
    "        linewidth=2,\n",
    "        marker=\"o\"\n",
    "    )\n",
    "\n",
    "# Aesthetics\n",
    "plt.title(\"Narrative Evolution of Emotion Groups\", fontsize=20, pad=40)\n",
    "plt.ylabel(\"Proportion\", fontsize=14)\n",
    "plt.xlabel(\"Saga\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=11)\n",
    "\n",
    "plt.legend(\n",
    "    title=\"Emotion Group\", \n",
    "    fontsize=12, \n",
    "    title_fontsize=13,\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.05),\n",
    "    ncol=len(melted_df[\"emotion_group\"].unique())\n",
    ")\n",
    "plt.grid(axis='y', linestyle='dotted', alpha=0.3)\n",
    "plt.gca().set_yticklabels([])\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that this is already looking great for our analysis on the sagas level. Now let's see it on the character level!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis of Emotions by Character\n",
    "\n",
    "As documented on the readme file, this may not work so well, as the character detection of the magiv2 transformer is a bit off most of the times. Either way, let's have a looksie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onepiecedata = pd.read_parquet(\"../outputs/character.parquet\")\n",
    "onepiecedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll use all functions from before, so this part is quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onepiecedata[\"emotions\"] = onepiecedata[\"text\"].apply(lambda x: classify_emotions(x.split(\".\")))\n",
    "# onepiecedata.to_parquet(\"../outputs/emotions_by_character.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onepiecedata = pd.read_parquet(\"../outputs/emotions_by_character.parquet\")\n",
    "onepiecedata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I think of it, I don't wish to expand this analysis to more than the straw hat crew. I could have saved some time on this process by filtering them before...\n",
    "\n",
    "Well, let's do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [\"Luffy\", \"Zoro\", \"Nami\", \"Usopp\", \"Sanji\", \"Chopper\", \"Robin\", \"Franky\", \"Brook\", \"Jinbe\"]\n",
    "\n",
    "onepiecedata = onepiecedata[onepiecedata[\"character\"].isin(characters)]\n",
    "onepiecedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_expanded = []\n",
    "for idx, row in onepiecedata.iterrows():\n",
    "    for emotion, count in row[\"emotions\"].items():\n",
    "        emotion_expanded.append({\n",
    "            \"character\": row[\"character\"],\n",
    "            \"emotion\": emotion,\n",
    "            \"count\": count\n",
    "        })\n",
    "\n",
    "emotions_df = emotions_df[emotions_df[\"emotion\"] != \"neutral\"]\n",
    "emotions_df = pd.DataFrame(emotion_expanded).fillna(0)\n",
    "emotions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column with grouped emotion labels\n",
    "emotions_df[\"emotion_group\"] = emotions_df[\"emotion\"].map(emotion_groups)\n",
    "\n",
    "# Group by character and emotion group, summing the counts\n",
    "grouped_emotions = (\n",
    "    emotions_df.groupby([\"character\", \"emotion_group\"])[\"count\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate total emotion counts per character\n",
    "total_per_saga = grouped_emotions.groupby(\"character\")[\"count\"].sum().rename(\"total\")\n",
    "\n",
    "# Merge totals back and compute proportional share\n",
    "grouped_emotions = grouped_emotions.merge(total_per_saga, on=\"character\")\n",
    "grouped_emotions[\"proportion\"] = grouped_emotions[\"count\"] / grouped_emotions[\"total\"]\n",
    "grouped_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table for the stacked bar plot\n",
    "pivot_df = grouped_emotions.pivot_table(\n",
    "    index=\"character\", \n",
    "    columns=\"emotion_group\", \n",
    "    values=\"proportion\", \n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# 2. Reindex pivot_df to reflect saga narrative order\n",
    "pivot_df = pivot_df.reindex(characters[::-1])\n",
    "\n",
    "# Ensure consistent order of columns\n",
    "pivot_df = pivot_df[[col for col in emotion_colors if col in pivot_df.columns]]\n",
    "\n",
    "# Plot horizontal stacked bar\n",
    "ax = pivot_df.plot(\n",
    "    kind=\"barh\",  # ← horizontal bar chart\n",
    "    stacked=True,\n",
    "    figsize=(14, 12),\n",
    "    color=[emotion_colors[col] for col in pivot_df.columns]\n",
    ")\n",
    "\n",
    "# Titles and labels\n",
    "plt.xlabel(\"Predominance of Emotion Group\", fontsize=15)\n",
    "plt.ylabel(\"\")\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Legend\n",
    "plt.legend(\n",
    "    title=\"Emotion Group\",\n",
    "    title_fontsize=15,\n",
    "    fontsize=13,\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.07),\n",
    "    ncol=len(pivot_df.columns)\n",
    ")\n",
    "\n",
    "# Layout and style\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "ax.set_xticklabels([])\n",
    "plt.grid(axis='x', linestyle='dotted', alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that is a bummer :<\n",
    "\n",
    "You may think: \"why is that\"\n",
    "\n",
    "By looking at how emotions are distributed on that visual, for each strawhat, there is not much we could say about their phrases on the narrative. Maybe due to wrong classifications during transcription or due to technicalities on the narrative so far. I was hoping to be able to differentiate them based on emotions, as I have a bias on believing Luffy would be the most joyful of them all. It sure looks like that, but statistically I don't think we could make this affirmation.\n",
    "\n",
    "Out of curiosity I remade this visual to each character on the analysis and nothing stood out. I gues that's it for characters then.\n",
    "\n",
    "If you liked my project so far and wish to contribute in any way or suggest me something to try, feel free to reach me out on linkedin! I don't have anything else planned for this as of the time I'm writing my last analysis.\n",
    "\n",
    "I'll be posting pdfs I've made for linkedin in the english language, exploring everything I've done so far and they will also be available on this repo. \n",
    "\n",
    "If you read everything through, `congrats!` For me (as I kept you here somehow) and to you for you curiosity! Until a next time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
